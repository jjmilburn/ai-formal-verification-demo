{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dafny Demo: LLM-Assisted Verification\n",
    "\n",
    "**Dafny** is a verification-aware programming language. You write code with annotations (preconditions, postconditions, loop invariants), and Dafny proves your code satisfies them.\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Show what verified Dafny code looks like\n",
    "2. Give an LLM unannotated code and ask it to add annotations\n",
    "3. Run the verification loop until it passes\n",
    "\n",
    "## Why Dafny?\n",
    "\n",
    "Dafny is a sweet spot for LLM + verification:\n",
    "- Syntax is familiar (C-like)\n",
    "- Annotations are inline with code\n",
    "- Error messages are specific and actionable\n",
    "- The [dafny-annotator](https://dafny.org/blog/2025/06/21/dafny-annotator/) tool showed LLMs can do this well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from src.llm_client import LLMClient\n",
    "from src.verifiers import DafnyVerifier\n",
    "from rich import print as rprint\n",
    "from rich.syntax import Syntax\n",
    "from rich.panel import Panel\n",
    "\n",
    "client = LLMClient()\n",
    "verifier = DafnyVerifier()\n",
    "\n",
    "print(\"‚úÖ Client and verifier ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Binary Search\n",
    "\n",
    "Binary search is a classic verification challenge. We need to prove:\n",
    "1. We never access out-of-bounds indices\n",
    "2. If we return an index, the element at that index equals the key\n",
    "3. If we return -1, the key isn't in the array\n",
    "\n",
    "### The Target: Fully Annotated Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../examples/dafny/binary_search.dfy') as f:\n",
    "    verified_code = f.read()\n",
    "\n",
    "rprint(Panel(Syntax(verified_code, \"csharp\", theme=\"monokai\"), title=\"‚úÖ Verified Binary Search\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify it actually passes\n",
    "result = verifier.verify(verified_code)\n",
    "print(f\"Verification: {'‚úÖ PASSED' if result.success else '‚ùå FAILED'}\")\n",
    "if not result.success:\n",
    "    print(result.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Challenge: Skeleton Code\n",
    "\n",
    "Now let's see the same code *without* annotations. This is what we'll ask the LLM to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../examples/dafny/binary_search_skeleton.dfy') as f:\n",
    "    skeleton_code = f.read()\n",
    "\n",
    "rprint(Panel(Syntax(skeleton_code, \"csharp\", theme=\"monokai\"), title=\"‚ö†Ô∏è Unannotated Skeleton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should fail verification\n",
    "result = verifier.verify(skeleton_code)\n",
    "print(f\"Verification: {'‚úÖ PASSED' if result.success else '‚ùå FAILED (expected)'}\")\n",
    "print(f\"\\nErrors:\\n{result.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Verification Loop\n",
    "\n",
    "Now we run the core loop:\n",
    "1. Ask LLM to add annotations\n",
    "2. Try to verify\n",
    "3. If it fails, show the error to the LLM\n",
    "4. Repeat until success or max attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_verification_loop(skeleton: str, max_attempts: int = 5):\n",
    "    \"\"\"Run the LLM + verifier loop.\"\"\"\n",
    "    code = skeleton\n",
    "    \n",
    "    for attempt in range(1, max_attempts + 1):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Attempt {attempt}/{max_attempts}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        # Ask LLM to generate/fix annotations\n",
    "        if attempt == 1:\n",
    "            print(\"\\nüì§ Asking LLM to add annotations...\")\n",
    "            response = client.generate_dafny_annotations(code)\n",
    "        else:\n",
    "            print(f\"\\nüì§ Asking LLM to fix based on error...\")\n",
    "            response = client.generate_dafny_annotations(code, error=result.error)\n",
    "        \n",
    "        code = client._extract_code(response.content)\n",
    "        print(f\"   Tokens used: {response.input_tokens + response.output_tokens}\")\n",
    "        \n",
    "        # Show the generated code\n",
    "        rprint(Panel(Syntax(code, \"csharp\", theme=\"monokai\", line_numbers=True), \n",
    "                     title=f\"LLM Output (Attempt {attempt})\"))\n",
    "        \n",
    "        # Verify\n",
    "        print(\"\\nüîç Running Dafny verifier...\")\n",
    "        result = verifier.verify(code)\n",
    "        \n",
    "        if result.success:\n",
    "            print(\"\\n‚úÖ VERIFICATION PASSED!\")\n",
    "            return code, attempt\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Verification failed:\")\n",
    "            print(result.error)\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Max attempts ({max_attempts}) reached without success\")\n",
    "    return code, max_attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the loop!\n",
    "final_code, attempts = run_verification_loop(skeleton_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Find Maximum\n",
    "\n",
    "Let's try another classic: finding the maximum element in an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../examples/dafny/max_array_skeleton.dfy') as f:\n",
    "    max_skeleton = f.read()\n",
    "\n",
    "rprint(Panel(Syntax(max_skeleton, \"csharp\", theme=\"monokai\"), title=\"FindMax Skeleton\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_code, attempts = run_verification_loop(max_skeleton)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Observations\n",
    "\n",
    "### What Makes This Work\n",
    "\n",
    "1. **Precise error messages**: Dafny tells us exactly which assertion failed and where\n",
    "2. **Structured task**: Adding annotations is well-defined‚Äîthe LLM doesn't need to reinvent the algorithm\n",
    "3. **Objective success criteria**: Either it verifies or it doesn't‚Äîno ambiguity\n",
    "\n",
    "### Failure Modes\n",
    "\n",
    "Sometimes the LLM:\n",
    "- Adds overly complex invariants that are correct but hard to prove\n",
    "- Gets stuck in a loop trying slight variations\n",
    "- Modifies the implementation instead of just adding annotations\n",
    "\n",
    "### FM-ALPACA Findings\n",
    "\n",
    "The [FM-ALPACA paper](https://arxiv.org/abs/2501.16207) found:\n",
    "- LLMs perform best on \"proof generation\" tasks (what we're doing here)\n",
    "- Fine-tuned models significantly outperform base models\n",
    "- Providing context from the same file improves results\n",
    "\n",
    "## Next: Lean4\n",
    "\n",
    "Continue to [03-lean4-demo.ipynb](03-lean4-demo.ipynb) to see the same problem tackled with a theorem prover."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
